---
sidebar_position: 4
slug: /
---

# Module 4 â€“ Vision-Language-Action (VLA)

This module explores Vision-Language-Action systems that combine language models, perception, and robotics to enable high-level autonomous humanoid behavior. The content is designed for CS/AI students with ROS 2, simulation, and robotic perception knowledge.

## What You'll Learn

- Vision-Language-Action paradigms and perception-to-action loops
- Voice-to-action interfaces using speech recognition and ROS 2
- LLM-based cognitive planning for robotic task decomposition

## Module Structure

This module is organized into three main sections that build upon each other:

- **VLA Paradigm**: Fundamental concepts of Vision-Language-Action systems
- **Voice-to-Action Interfaces**: Implementation of voice-driven robotic control
- **LLM Planning**: Advanced cognitive planning techniques for robotics

## Learning Path

We recommend following this learning path for optimal understanding:

1. [VLA Paradigm Overview](./vla-paradigm/overview.md) - Start with fundamental VLA concepts
2. [Voice-to-Action Interfaces](./voice-to-action/interfaces.md) - Explore voice-driven robotic control
3. [LLM-Based Cognitive Planning](./llm-planning/cognitive-planning.md) - Learn advanced planning techniques

[Start with VLA Paradigm Overview](./vla-paradigm/overview.md) | [View Glossary](./glossary.md)